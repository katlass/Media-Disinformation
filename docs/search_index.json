[["index.html", "Disinformation, social media, and political orientation Chapter 1 Introduction", " Disinformation, social media, and political orientation Kate Lassiter, Renato Russo 2022-12-15 Chapter 1 Introduction This topic stands out to us as students who have grown up in the age of fake news and social media. When we receive information, we don’t always go out of our way to fact check every headline. Our own internal bias may often lead us to accept false information as true, simply by cherry picking facts we already believe to be true. This is known as confirmation bias. This analysis sets off to study the effects of this bias, the susceptibility of individuals to fake, sensationalized articles, and the differences in open minded thinking across different political affiliations. For example, one common assumption is that very conservative Republicans are not open minded and are unwilling to change their ways of thinking on important topics, such as religion. Another assumption could be that very liberal individuals like to play the devil’s advocate and take the contrarian point of view. So, they read articles that are very conservative for a longer than average time in order to piece together a narrative of why the opposite might be true. We are hoping to test some of these sweeping generalizations about certain political factions. The aim of this analysis is to give a greater context as to whether behavior in this domain truly varies based on political agenda or whether it’s other factors that make us more or less susceptible to fake news and changing our opinions. If political extremism really plays a large role in our susceptibility to fake news, this shows that those who are most strongly republican or democratic are the ones being prayed upon by the ever increasing media circus in the United States. Many of the news stories played on cable television contain truth and lies, and the people who are most likely to watch the news are those who have the strongest political opinions. We are constantly bombarded by fake news, and this may lead to an increasing divide between political parties, especially among those with more extreme views on either end of the spectrum. When the country is divided over these fundamental issues, and people are constantly shown false news that push them further to the extreme, this poses a major challenge to society. In order to address and combat confirmation bias and fake news, we must first identify what makes us most susceptible and attempt to modify our behaviors. "],["proposal.html", "Chapter 2 Proposal 2.1 Research topic 2.2 Data availability", " Chapter 2 Proposal 2.1 Research topic In this project, we chose to address one of the many facets of the intersection between politics and disinformation: the association between confirmation bias related to political orientation and susceptibility to fake news. The issue of disinformation is age-old, but has gained more attention in the past few years, especially in relation to the effects it allegedly has on politics. Those effects are associated with electoral campaigns and to media coverage of the pandemic (Calvillo et al., 2020). As well as its effects, it also seems to be critical to understand the causes to disinformation. Indeed, due to the increasing relevance of the phenomenon, research in many fields seek explanations to the causes and potential solutions to the issue. In this project, we look at the topic through the lens of cognitive psychology, the field of psychology interested in “the way in which the human mind receives impressions from the external world and interprets the impressions thus received” (Moore, 1939). Recent research on disinformation in the field of cognitive psychology includes studies that investigate the effects of “inoculation” against fake news. According to that hypothesis, individuals become less susceptible to disinformation if they are exposed to small doses of fake news in a controlled setting, similarly to what happens with vaccines used against viruses (Compton et al., 2021). With this project, we set out to explore other aspects that are potentially associated with susceptibility to disinformation, namely those related to political orientation. – Calvillo, D. P., Ross, B. J., Garcia, R. J., Smelter, T. J., &amp; Rutchick, A. M. (2020). Political ideology predicts perceptions of the threat of COVID-19 (and susceptibility to | fake news about it). Social Psychological and Personality Science, 11(8), 1119-1128. Moore, T. V. (1939). Cognitive psychology. Compton, J., van der Linden, S., Cook, J., &amp; Basol, M. (2021). Inoculation theory in the | post‐truth era: Extant findings and new frontiers for contested science, misinformation, | and conspiracy theories. Social and Personality Psychology Compass, 15(6), e12602. 2.2 Data availability Because we are interested in biases and misinformation, we started looking for academic studies that address that topic. With that in mind, we started browsing data sets available at the Open Science Framework database. The framework is an initiative of the Center for Open Science that makes available data sets and other content related to academic research following open source principles. We used the portal’s search tool to identify data sets using the search query “disinformation + politics” and found a few projects related to our topic. We decided upon one that addresses one specific facet of disinformation and media consumption: The fake news and confirmation bias dataset from the Confirmation bias in fact checking political claims research project. After discussion among the team members, we understood that dataset offers a diversity of types across its 53 dimensions and is accompanied by a comprehensive dictionary. It also presents 247 observations, which we believe will enable the identification of patterns across groups such as experiment conditions, political party affiliation, and gender. The data source was obtained by a team of researchers led by Dr. Dustin Calvillo, a professor and researcher at California State University San Marcos. The data set is hosted at OSF open source data base. More information is needed in relation to the specific research design, but it is possible to infer that the data was obtained through experiment and surveys. The experiment consisted of exposing individuals to fake and real headlines that subjects had to evaluate and varied between conservative and progressive political orientation. The survey includes psychometric dimensions, such as an actively open-minded thinking scale and attitude toward fact-checking. The data is available as an .sav file, and the codebook (data dictionary) as a .pdf file. The .sav file containing one observation per row and one variable per column. The data can be downloaded directly from the website, and can be imported to R using the read_sav function in the haven package. In case of questions about the data, Dr. Calvillo’s contact information isavailable on his profile pages both on OSF’s and CSU’s websites. The data has seemingly good overall quality. Apparently, NAs have been removed prior to being made available as an open source data set. The data dictionary has an apparent good quality too, with short explanations to all variables. One possible challenge is that further explanation to variables will need to be looked up in related literature – this can be time-consuming, but, at first glance, papers written by the team contain well-documented bibliography. "],["data.html", "Chapter 3 Data 3.1 Sources 3.2 Cleaning / transformation 3.3 Missing value analysis", " Chapter 3 Data 3.1 Sources Data used in this project was originally collected by a team based in California State University San Marcos, and led by Dr. Dustin Calvillo. A visit to his Google Scholar profile indicates that his recent research has focused on employing quantitative methods from cognitive psychology to the study of contemporary communication phenomena, especially mis- and disinformation. This specific data set – the fake news and confirmation bias – is part of the Confirmation bias in fact checking political claims research project. There is a similar dataset in the same project, which is apparently a previous version of the one used in this project (suffixes x1 and x2, respectively. Although both datasets share the same variables, a few are present in one and not in the other. For example, “disconfirm_bias” is present in x1, but not in x2; the opposite is true for “ideology” and “condition” (referring to “fact-checking conditions”). More clarification was requested via email to the principal investigator, but we are assuming that x2 is a more refined version of x1, so we opted to use the former for our analysis. The dataset has 247 observations of 53 variables, all of which are “double.” However, the data dictionary indicates that many of those represent other types of data and have possibly been transformed by the authors before publishing the data set. For example, the variable pol_party originally has values “Democrat” and “Repubican,” which are now represented by 1 and 2, respectively; gender originally accepted values “female,” “male,” “other,” and “decline to state,” and now are coded from 1 to 4, in that order. Others are originally integers, for example, t_concordant, which represents “total number f fact-checks for true politically concordant headlines.” The same is true for the Likert-type scale variable ideology, whose rating ranges from 1 to 7, “from extremely liberal to extremely conservative.” A third class that was present in the original dataset was “time,” the case, for example, of T_dem_rt (“mean reading time for true pro-liberal headlines”). Lastly, there are 2 variables that gorup other variables: CRT```` is the “total number of CRT items answered correctly” (0-6); andAOT```, which is the mean of items in the “active open-minded thinking” (1-5). One possible challenge that may arise as we start the exploratory data analysis is the transformation of time values form doubles to time and date format. In terms of missing values, although the data set seems to have been pre-processed in order to exclude NAs, there are 4 observations that contain missing values (1 NA each). 3.2 Cleaning / transformation #Add participant id dataset$participant_id=1:nrow(dataset) #Define a factor converter to convert numeric columns to factors factor_converter = function(data,columns){ for (x in columns){ data[,x]=as.factor(data[,x]) data[,x]=addNA(data[,x]) } return(data) } First, the data set is loaded in using the package Haven’s read_sav() function, as the data is stored as a .SAV file. This type of file is outputted by the application SPSS. It is then converted to a data frame for easier use. The authors of this study clearly spent time preparing their data for publishing and little transformation was needed. Upon examination, there are many categorical variables which have already been modified to be numeric, ordinal variables. However, they are still numeric and plotting in ggplot2 often requires ordinal variables to be in factor form. As such, many columns will need to be converted to factors. This is done with a custom function factor_converter() which takes columns and a data set as a inputs and creates factor level variables, including an NA factor. One option to explore is modifying some of these ordinal variables to be binary, as using ordinal scale on data that shouldn’t be ranked can obscure true relationships. Given this data relates to individual participants and their responses in the study, a new variable participant_id has been added to the data set to uniquely identify each participant. This allowed us to track whether any anomalies were related to specific individuals. For now, this allows us to move straight into exploratory data analysis. 3.3 Missing value analysis The authors mention that missing values have been removed prior to the data being made publicly available. However, there do seem to be a few missing observations present in the data. First we present the four variables that are missing in the data set and the overall distribution of scores for participants on these questions. Overall, open minded thinking questions one and four received high scores among the majority of participants, while scores were slightly more widespread for question nine. The overall score for participant attitude toward fact checkers based on question three is also high. We now show below only the participants and columns that contained missing values to see if there were any similarities: Variables: aot: mean rating for 11 actively open-minding thinking questions (possible range of 1-5) aaf: mean rating for three attitude toward fact-checkers questions (possible range of 1-7) fc_att3: response to the third item on the attitudes toward fact-checkers scale (possible range of 1-7) aot01: response to the first item on the actively open-minded thinking scale (1-5) aot04: response to the fourth item on the actively open-minded thinking scale (1-5) aot09: response to the nineth item on the actively open-minded thinking scale (1-5) ## participant_id aot01 aot04 aot09 fc_att3 ## 1 4 4 4 NA ## 197 3 4 NA 3 ## 213 NA 4 2 5 ## 215 4 NA 2 3 All four participants had a different column item missing, meaning they all had one blank response to a different question. This leads us naturally to investigate further whether these participants are similar across some other dimension. ## id aot01 aot04 aot09 fc_att3 gender pol_party ideology condition age aot aaf ## 1 4 4 4 NA female democrat 2 easy 34 3.64 4.50 ## 197 3 4 NA 3 female republican 7 difficult 69 3.60 3.33 ## 213 NA 4 2 5 male republican 5 difficult 26 3.40 4.33 ## 215 4 NA 2 3 female republican 7 difficult 77 3.30 2.33 There seems to be little in common among the four participants at first glance. However, all four score roughly a 3.5 on the AOT mean rating for 11 actively open minded thinking questions. Considering the scale goes up to only five, this means these participants were on the higher end in terms of their willingness to explore alternate ways of thinking. Considering aot01, aot04, and aot09 are all questions intended to judge open minded thinking, it’s possible that these participants believed the question was neutral, that it was outside of their realm of knowledge, or that they just couldn’t think of a black and white answer so they left it blank. But, it must be noted that all three participants missing an open minded thinking question are republican and two have the highest ideology rating, meaning they are extremely conservative (participants 197 and 215). Perhaps a question was asked that disagreed with them morally, such as abortion or the death penalty. This may have led them to give no response to this question. This theory can better be confirmed by the fact that participant 215 was not very open minded in regards to the ninth question, aot09, and only scored a 2 out of 5. Participant 197 chose not to answer this question at all. This is further backed up by the fact that responses to question nine are more widespread than the other questions, it could easily be a polarizing question between parties. Both also scored relatively low on questions intended to gauge the participants attitudes towards fact checkers, 2 and 3 out of 7 respectively. Maybe they felt uncomfortable sharing their views with the the proctor. Another idea could be that these three participants simply had a hard time answering these questions because the fact checking conditions were difficult. This may have made them more guarded in terms of their willingness to answer these type of questions. As for participant 1, she is a democrat who is extremely liberal with an ideology score of 2, with one being extremely liberal. fc_att3 was one of the questions given to participants to gauge their attitudes toward fact checkers. Overall, participant 1 scored relatively high in terms of her attitude toward fact checkers, with an aaf of 4.5/7. So it seems that overall she felt positively about fact checkers, but maybe she just didn’t feel that question was appropriate. All four are still valid participants to include in the study considering all So it makes sense the author is left on itother questions were answered. "],["results.html", "Chapter 4 Results 4.1 Overview of respondents 4.2 Political Concordance vs Number of Fact-checks 4.3 Total Number of Fact Checks 4.4 Actively Open-minded Thinking and Political Affiliation 4.5 Cognitive Reflection Test Scores 4.6 Reading Time for True and False, Concordant and Discordant Headlines according to Political Affiliation 4.7 Confirmation Bias across Ideologies and Political Party Affiliation 4.8 Fact-checking Attitude and Overall Reading Time 4.9 Reading time for concordant and discordant headlines 4.10 Actively Open-minded Thinking Mean Rating across Ideologies", " Chapter 4 Results 4.1 Overview of respondents The charts above show that the sample is relatively balanced in terms of their political affiliation and ideology. About half of the sample identifies themselves as Democrats, half as Republicans. As for ideology, despite the difference in the extremes (the number of extremely liberals is about double that of extremely conservatives), the total numbers are rather similar at each side (124 to the side of liberals; 123 to the side of conservatives). As for gender, we can see a large gap between female and male respondents (166 x 178, respectively). Respondents who chose “other” or opted to not disclose their genders account for a total of 3 individuals, and are not shown in this chart (even if those categories were “topcoded”, the height of the bars would be very low). The researchers also achieved an important symmetry between the two experimental conditions of fact-checking: 125 were assigned to “easy;” 122 to “difficult.” 4.2 Political Concordance vs Number of Fact-checks The chart above shows the amount of fact-checks conducted by individuals according to their political affiliation: Republicans are in red, Democrats are in blue, and the colors have a level of transparency (alpha). Each of the individual charts refers to concordance or discordance between the information presented and the individual’s political perspective. Therefore, “True concordant” refers to information that is true and aligned with the respondent’s political perspective; “False concordant” refers to information that is false and concordant with the individual’s political perspective; the same logic applies to the other two categories. Some of the charts show interesting patterns. For example, the top-right plot (True concordant) shows an intriguing symmetry. The number of Republicans who perform 0 fact check on true concordant news is higher than that of Democrats. The same is true for 1 and 2 fact-checks. The numbers are very similar on the other side of the chart, but now Democrats outnumber Republicans for 4, 5, and 6 fact-checks. An almost opposite pattern arises for False discordant headlines, although with less stark symmetry. Here, Republicans are also the most representative group performing 0 fact-checks, and Democrats are majority among those performing 6 fact-checks. The number of individuals are the same across political parties for 1 and 5 fact-checks, and Democrats are majority for 2 and 3 fact-checks. 4.3 Total Number of Fact Checks The chart above shows that there are no salient trends in terms of fact-checking when comparing ideologies. Different features have been tested for the visualization total fact-checking, and no salient pattern has been identified. For example, we attempted to include gender as a faceting dimension, and the chart did not reveal any relevant patterns, as seen below. 4.4 Actively Open-minded Thinking and Political Affiliation The boxplots show the score for actively open-minded thinking – “a general prescriptive theory that defines a standard or norm for all thinking, with emphasis on its role in the judgment of the thinking of others, and in maintaining appropriate confidence” (Baron, 2019). The chart shows that the most sensitive difference in that measure appears when comparing male participants across political affiliations: the median score for Democrat men is above the third quartile for Republican men. It is also noteworthy that all quartiles for Republican men are below their counterparts for Democrat men, although the minimum and maximum in one group are not very distant to the same measures in the other. Similarly, difference between quartiles also appear among women, and the median for female Democrats is very close to Republicans’ third quartile. The comparison between the same gender and across political parties does not show substantial differences. In the comparison across genders, it is important to note that the sample size for women is about double the size for men. 4.5 Cognitive Reflection Test Scores The doubledecker chart shows the total number of items responded correctly in the CRT scale (“Cognitive Reflection Test,” Frederick, 2005). This test was originally employed as an alternative to IQ tests in the contexts of business and economics, but has been adopted by many scholars investigating political motivations of misinformation in the past years. The width of “spines” in the chart points to a tendency of increase in size of the groups for higher numbers of correct answers. This is evident, for example, by the width of the spine for 6 correct answers. That is, when grouped by the number of correct answers, the largest segment is the one with participants who answered all questions correctly. There is a fair amount of variance in terms of affiliation to political parties across numbers of correct answers, except in the lower end, where Republicans prevail in this specific sample. Although no conclusive evidence can be drawn from this chart, Republicans are the largest group among those who answered only 0 and 1 questions correctly. The prevalent group varies considerably from 2 to 6 correct answers. 4.6 Reading Time for True and False, Concordant and Discordant Headlines according to Political Affiliation The charts above show the time spent by individuals reading headlines during the experiment. Each of its quadrants shows a combination between veracity of headlines and the headline’s bias. One striking tendency shown by the chart are the peaks in counts for Republicans in each chart. For example, for true pro-liberal headlines, about 70 Republicans had about 2-minute reading time against about 18 Democrats in the same number of minutes. Similar trends can be found in other charts. Apparently, reading times for Democrats appear to be slightly more sparse than those for Republicans. It is also noteworthy that peaks for Democrats appear very early in the time count for all combinations (in all, the peaks appear at about 2 minutes). For Democrats, the distribution seem to be multimodal in true pro-conservative and false pro-liberal headlines. 4.7 Confirmation Bias across Ideologies and Political Party Affiliation The chart shows the measure for confirmation bias (calculated as the total of concordant fact-checks minus discordant fact-checks) across ideologies and political affiliations. The largest difference among groups observed is between Democrats and Republicans. Here, again, there is a considerable difference in median score, which is higher among Democrats. This group, however shows the lowest minimum scores – and the highest maximum scores. The median for Republicans is close to the first quartile of Democrats, and the third quartiles for both groups are similar. Looking at the scores across ideologies, we see that the lowest medians are located in the extremes (the two most extreme liberals and the three conservative groupings all share a median close to 0). The group with least variability is the one in the middle (classified in other analysis of the same dataset as conservative), which has a median very similar to its neighboring group of liberals. 4.8 Fact-checking Attitude and Overall Reading Time The chart above shows a scatter plot for overall reading time against fact-checking attitude, a measure of mean rating for three attitude toward fact-checkers questions. There is a visible concentration of observations in the top-right corner, apparently most pronounced among Republicans. In that corner are observations with low overall reading time and high fact-checking attitude. Accordingly, observations for Republicans are concentrated in the top half of the chart, whereas the ones for Democrats seem to be slightly more spread across the two halves. It is also remarkable that there is an apparent straight line for Democrats along about 4 minutes which could not be observed in the chart above that shows a segmentation by concordance with the headlines. 4.9 Reading time for concordant and discordant headlines These scatter plots show the relationship between two measures of reading time, for concordant headlines (pro-liberal for Democrats; pro-conservative for Republicans), and discordant headlines (pro-conservative for Democrats; pro-liberal for Republicans). An interesting aspect of this chart is that it is a way of measuring biases from individuals: in case of absence of biases, one could expect that reading times for both discordant and concordant headlines were positively correlated – individuals would spend similar amounts of time in headlines, no matter if those are concordant or discordant. For both groups, the distributions seem to be positively correlated, however moderately. Also, the charts show slightly different relationships for Democrats and Republicans. For Democrats, the bulk of observations is concentrated in the range below 20 seconds in each axis. Also, the correlation seems to be more accentuated in the bottom left of the chart, and points start to become more scattered above 10 seconds in each axis, following a less defined pattern from them on. For Republicans, the bulk of observations lies in the region below 20 seconds for concordant headlines and in the vicinity of 10 seconds for discordant headlines. The points follow an even more nuanced relationship in comparison with that for Democrats, and the pattern of correlation is less pronounced. 4.10 Actively Open-minded Thinking Mean Rating across Ideologies The grid of ridgeline plots shows comparisons for constructs Actively Open-Minded Thinking across levels of allegiance to ideologies and political party affinity. The top chart shows a unimodal distribution for almost all segments of ideology, in general, negatively skewed. This points to generally high scores in this 0-5 scale that is the mean score for 11 questions. One interesting exception is the level 3 (somewhere around “slightly liberal”). In that segment, the distribution is bimodal, with the highest peak around the score 4, and a lower peak around the score 3, a value for which there are very few observations in all other groups. Another very low score that is highlighted in the top chart is in the “extremely conservative” group, right below 2 points in the scale. When we look at differences across genders, it is possible to see both “mirrors” of those general trends and further nuances. For example, the general trend for extremely conservative is reflected among female respondents, but not among males. In the latter, the distribution presents at least three peaks (around 3, 3.6, and 4.5, although the two lower ones might reflect a rounding pattern). The general, bimodal pattern among the slightly liberal, by its turn, is also present in both genders. Other patterns appear among men that is not present among women or in the distribution for the whole sample. First, the bimodality of the most moderate group of conservatives (the fourth horizontal line in each plot). Among male, the highest peak in that group appears close to 4.5, above the peaks for female and for the whole sample (around 3.8), although the latter has a subtle bump to the right of its peak, which might be partially explained by the peak among mean. A similar pattern appears among the extremely liberal: apparently a lower peak towards the right end of the plot has a timid effect on the plot for the entire sample. References: Baron, J. (2019). Actively open-minded thinking in politics. Cognition, 188, 8-18. Frederick, S. (2005). Cognitive reflection and decision making. Journal of Economic perspectives, 19(4), 25-42. "],["interactive-component.html", "Chapter 5 Interactive component", " Chapter 5 Interactive component No analysis can be complete without a review of deviations from the norm. In this case, mean reading times for politically discordant headlines (pro-conservative for Democrats, pro-liberal for Republicans) are normalized across participants to identify individuals that read politically contrasting articles for a longer than average time. In the interactive above, the dashed gray line represents the mean and each red dashed line represents one standard deviation above the mean. No observations fall below one standard deviation of the mean. Each circle is colored according to the participant’s political ideology. Unsurprisingly, it tends to be participants on the extremes of the ideology spectrum, either extremely liberal (score 1-2) or extremely conservative (6-7), that spend the longest time reading headlines that disagree with their political agenda. This is often the very foundation of news, to sensationalize people with headlines that shock them or go against there point of view. This leads them to read articles for longer, discuss them with friends, and often believe the article on face value without doing any further research. We present here a chart of all participants with reading times greater than or equal to one standard deviation above the mean. These are the points that are at least past the first dashed red line in the interactive above. These participants spent longer than “normal” reading these headlines. Of the 30 participants that met this criteria, 23 were either extremely liberal or extremely conservative. Further, of the 14 participants that had reading times two deviations above the average, 11 were either extremely liberal or extremely conservative. This reinforces the belief that those who are strongly opinionated are more drawn to articles that conflict with their beliefs than someone who is more moderately inclined. *Note: A few alternate methodologies were tested for this interactive. Originally, min-max scaling was applied to all discordant reading times, and this value from zero to one was multiplied by the width of the image to determine final circle position. However, the results didn’t look accurate as the majority of points were to the far left of the screen due to a few large outliers in reading time. This is because when applying min-max scaling to the entire data set, the outliers caused the majority of the values to be smashed close to zero. Considering the outliers, the next approach was to have the median considered as the middle of the screen instead. Then, min-max scaling was applied to the lower and upper halves of the data separately, and this 0 to 1 value was then modified such that 0.5 becomes the maximum for the lower half and minimum for the upper half. This was multiplied by the width of the image to get the final circle position. While this still showed the extreme points, it was not as clear to what extent the points were deviations from the norm. This led us to the final methodology of normalizing the values, multiplying this normalized value by the number of pixels selected to represent one deviation, and then adding this number to the center of the screen (half the image width) which represents the mean. This exercise was fun and important because it showed a real example of when outliers cause poor data transformations. Code for the alternate data cleaning can be found here: interactive_prep_median.R "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion In this project, we explored an open-access dataset that contains data related to misinformation, largely in relation to political affiliation and ideology. This data shows clear signs of proper experimental design: the sample is balanced in terms of political orientation and ideology, observations with a great number of missing values were omitted, and there is symmetry in fact-checking conditions administered to participants. However, the sample was not quite balanced in terms of gender and, besides the absence of non-gender conforming individuals, there were nearly double the number of women as men in the sample. Future work could explore the experimental nature of the data in more depth. Why were more women included than men? Was this intentional? Certainly gender plays a large role in politics and the argument could be made that this biases the data. If they had to include that many more women to get a balanced profile for political affiliation and ideology, you could say that many of the differences found in the study were related directly to different biases between men and women. Therefore, could this study be pushed further and include more male participants? One of the most striking pieces of evidence found in analysis was the difference between Republicans’ and Democrats’ “actively open-minded thinking (AOT)” especially among male respondents. This AOT scale was originally created in the context of corporate decision making, but has become widespread in recent scholarly work on misinformation and fake news (e.g., Swire et al., 2017; Maertens et al., 2021). Our box plots showed that even the median AOT for democratic men was above the third quartile of Republican men. This means that around 50% of democratic men scored higher in open minded thinking than 75% of Republican men. This confirms speculation that many Republican men are very set in their political thinking. Though we find a similar pattern for women in these plots, it is not of the same scale. Another remarkable feature of the sample was found with the boxplot for “confirmation bias”, a measure of individuals’ fact-checking disposition. It shows that Democrats and Republicans have similar scores for some quartiles, but not all. The two groups have similar third quartiles, but have a marked difference in medians. The median score for Republicans is similar to the first quartile of Democrats. So 50% of republicans score less than or equal to 25% of Democrats. This shows that Republicans in this sample were less likely to fact check headlines that were already concordant with their ideologies. They were willing to believe them to be true because they align with what they already believed to be true, resulting in confirmation bias. When the same box plots are split among ideologies, the most remarkable feature is the similarity among medians for the all groups (close to 0), except for the two most moderate segments among liberals and conservative. For these two groups, the medians are considerably above the others, indicating individuals who are not particularly politically affiliated were much more inclined to fact check articles when they were unsure they were accurate. Limitations of our analysis include lack of access to some methodological decisions. For example, although the ideology scale had 7 items, conservative and liberals are asymmetrically distributed along those 7 values (liberals are those who choose 1 to 3; conservatives, 4 to 7). Due to the apparent robustness of the study design, we assume that the researchers had clear motivations for that asymmetrical split, but the dataset documentation does not cover that aspect. Having a thorough understanding of the underlying experimental design is crucial for a clearer analysis of this study. In the future, further analysis of statistically significant differences among groups can be conducted, following clues offered by the plots. For example, the difference in median AOT scores among male Democratic and Republian participants would help strengthen our claims. This can be done in addition to the aforementioned attention to the experimental nature of the dataset. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
